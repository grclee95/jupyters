{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0a86fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import platform\n",
    "from matplotlib import font_manager, rc\n",
    "\n",
    "#그래프에서 한글을 사용하기 위해서 설정\n",
    "if platform.system() == 'Darwin':\n",
    "    rc('font', family=\"AppleGothic\")\n",
    "elif platform.system() == 'Windows':\n",
    "    font_name = font_manager.FontProperties(fname='c:/Windows/Fonts/malgun.ttf').get_name()\n",
    "    rc('font', family=font_name)\n",
    "\n",
    "#그래프에 음수를 사용하기 위해서 설정\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e8e0812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    mpg  cylinders  displacement  horsepower  weight  acceleration  \\\n",
      "0  18.0          8         307.0       130.0  3504.0          12.0   \n",
      "1  15.0          8         350.0       165.0  3693.0          11.5   \n",
      "2  18.0          8         318.0       150.0  3436.0          11.0   \n",
      "3  16.0          8         304.0       150.0  3433.0          12.0   \n",
      "4  17.0          8         302.0       140.0  3449.0          10.5   \n",
      "\n",
      "   model year  origin                       name  \n",
      "0          70       1  chevrolet chevelle malibu  \n",
      "1          70       1          buick skylark 320  \n",
      "2          70       1         plymouth satellite  \n",
      "3          70       1              amc rebel sst  \n",
      "4          70       1                ford torino  \n"
     ]
    }
   ],
   "source": [
    "#auto-mpg 데이터를 가져와서 horsepower의 이상치를 제거하고 숫자 타입으로 변환\n",
    "#데이터 읽어오기\n",
    "auto_mpg = pd.read_csv('./data/auto-mpg.csv', header=None)\n",
    "#컬럼 이름 설정\n",
    "auto_mpg.columns = ['mpg', 'cylinders', 'displacement', 'horsepower', 'weight',\n",
    "                   'acceleration', 'model year', 'origin', 'name']\n",
    "#horsepower 열의 자료형을 실수로 변경\n",
    "#?를 None으로 치환하고 제거한 후 자료형 변경\n",
    "auto_mpg['horsepower'].replace('?', np.nan, inplace=True)\n",
    "auto_mpg.dropna(subset=['horsepower'], axis=0, inplace=True)\n",
    "auto_mpg['horsepower']=auto_mpg['horsepower'].astype('float')\n",
    "print(auto_mpg.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d598a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     저출력  보통출력  고출력\n",
      "0      0     1    0\n",
      "1      0     1    0\n",
      "2      0     1    0\n",
      "3      0     1    0\n",
      "4      0     1    0\n",
      "..   ...   ...  ...\n",
      "393    1     0    0\n",
      "394    1     0    0\n",
      "395    1     0    0\n",
      "396    1     0    0\n",
      "397    1     0    0\n",
      "\n",
      "[392 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "#원 핫 인코딩\n",
    "#1.horsepower 특성을 범주형으로 추가 - 3개의 영역으로 구분\n",
    "#3개의 구간으로 구분해서 개수와 경계값을 리턴받아서 저장\n",
    "\n",
    "count, bin_dividers = np.histogram(auto_mpg['horsepower'], bins=3)\n",
    "\n",
    "#범주형의 형태로 생성\n",
    "auto_mpg['hp_bin'] = pd.cut(x = auto_mpg['horsepower'],\n",
    "                           bins = bin_dividers,\n",
    "                           labels=['저출력', '보통출력', '고출력'],\n",
    "                           include_lowest = True)\n",
    "\n",
    "#print(auto_mpg[['horsepower', 'hp_bin']].head())\n",
    "\n",
    "#원 핫 인코딩 수행 - 값이 3종류이므로 3개의 특성이 만들어지고 값은 하나만 1\n",
    "horsepower_dummies = pd.get_dummies(auto_mpg['hp_bin'])\n",
    "print(horsepower_dummies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c9b620f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " ...\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]]\n",
      "['고출력' '보통출력' '저출력']\n"
     ]
    }
   ],
   "source": [
    "#하나의 특성을 원 핫 인코딩해주는 클래스\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "one_hot = LabelBinarizer()\n",
    "print(one_hot.fit_transform(auto_mpg['hp_bin']))\n",
    "\n",
    "#이름을 확인\n",
    "print(one_hot.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b043f11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 1 0 0 0]\n",
      " [0 1 0 0 0 1 0]\n",
      " [1 0 0 1 0 0 0]\n",
      " [0 0 0 1 1 0 0]\n",
      " [0 0 1 0 0 1 0]\n",
      " [0 0 0 0 0 1 1]]\n",
      "['C#' 'C++' 'Go' 'Java' 'Kotlin' 'Python' 'R']\n"
     ]
    }
   ],
   "source": [
    "# 2개 이상의 특성을 가지고 원 핫 인코딩\n",
    "# 2개 이상의 1이 등장할 수 있습니다.\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "multi_features = [(\"Java\", \"C++\") , (\"C++\", \"Python\"), (\"Java\", \"C#\"), \n",
    "                  (\"Java\", \"Kotlin\"), (\"Python\", \"Go\"), (\"Python\", \"R\")]\n",
    "\n",
    "one_hot_multiclass = MultiLabelBinarizer()\n",
    "print(one_hot_multiclass.fit_transform(multi_features))\n",
    "print(one_hot_multiclass.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "98316f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Score  encoder\n",
      "0    저조        1\n",
      "1    보통        2\n",
      "2    보통        2\n",
      "3    저조        1\n",
      "4    우수        3\n",
      "5  매우우수        4\n"
     ]
    }
   ],
   "source": [
    "#순서가 의미를 갖는 경우 - replace 함수 이용\n",
    "df = pd.DataFrame({\"Score\":[\"저조\", \"보통\", \"보통\", \"저조\", \"우수\", \"매우우수\"]})\n",
    "scale_mapper = {\"저조\":1, \"보통\":2, \"우수\":3, \"매우우수\":4}\n",
    "df['encoder'] = df['Score'].replace(scale_mapper)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5ab8f396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0.]\n",
      " [2. 2.]\n",
      " [0. 1.]]\n",
      "[array(['High', 'Low', 'Normal'], dtype='<U21'), array(['10', '15', '20'], dtype='<U21')]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "features = np.array([[\"Low\", 10], [\"Normal\", 20], [\"High\", 15]])\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "print(ordinal_encoder.fit_transform(features))\n",
    "print(ordinal_encoder.categories_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "14c4a7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1.]\n",
      "[[ 0.    0.87  0.31]\n",
      " [ 1.   -0.67 -0.22]]\n",
      "[[ 0.    0.87  0.31]\n",
      " [ 1.   -0.67 -0.22]\n",
      " [ 0.    2.1   1.45]\n",
      " [ 1.    1.18  1.33]\n",
      " [ 0.    1.22  1.27]\n",
      " [ 1.   -0.21 -1.19]]\n"
     ]
    }
   ],
   "source": [
    "#분류 모델을 이용한 결측값 대체\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#훈련 데이터 생성\n",
    "X = np.array([[0, 2.10, 1.45], [1, 1.18, 1.33], [0, 1.22, 1.27], [1, -0.21, -1.19]])\n",
    "\n",
    "#예측에 사용할 데이터\n",
    "X_with_nan = np.array([[np.nan, 0.87, 0.31], [np.nan, -0.67, -0.22]])\n",
    "\n",
    "#KNN 학습기 생성\n",
    "clf = KNeighborsClassifier(3, weights='distance')\n",
    "#첫번째 데이터를 label로 하고 나머지 데이터를 feature로 설정해서 훈련\n",
    "trained_model = clf.fit(X[:, 1:], X[:, 0])\n",
    "#예측\n",
    "imputed_values = trained_model.predict(X_with_nan[:, 1:])\n",
    "print(imputed_values)\n",
    "\n",
    "#예측한 데이터와 원본 데이터를 합치기\n",
    "X_with_imputed = np.hstack((imputed_values.reshape(-1, 1), X_with_nan[:, 1:]))\n",
    "print(X_with_imputed)\n",
    "\n",
    "#결측치를 대체한 데이터와 훈련에 사용한 데이터 합치기\n",
    "result = np.vstack((X_with_imputed, X))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4bdadbcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  nan  0.87  0.31]\n",
      " [  nan -0.67 -0.22]\n",
      " [ 0.    2.1   1.45]\n",
      " [ 1.    1.18  1.33]\n",
      " [ 0.    1.22  1.27]\n",
      " [ 1.   -0.21 -1.19]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.  ,  0.87,  0.31],\n",
       "       [ 0.  , -0.67, -0.22],\n",
       "       [ 0.  ,  2.1 ,  1.45],\n",
       "       [ 1.  ,  1.18,  1.33],\n",
       "       [ 0.  ,  1.22,  1.27],\n",
       "       [ 1.  , -0.21, -1.19]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#가장 많이 나오는 데이터로 대체\n",
    "from sklearn.impute import SimpleImputer\n",
    "X_complete = np.vstack((X_with_nan, X))\n",
    "print(X_complete)\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "imputer.fit_transform(X_complete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3f5fc1ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 1), match='1'>\n",
      "None\n",
      "<re.Match object; span=(0, 2), match=' 1'>\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "#매칭 여부를 확인\n",
    "match = re.match('[0-9]', '1234')\n",
    "\n",
    "#패턴에 일치하는 데이터가 있으면 Match 객체를 리턴하고 없으면 None 리턴\n",
    "print(match)\n",
    "\n",
    "match = re.match('[0-9]', 'abc')\n",
    "print(match)\n",
    "\n",
    "match = re.match('\\s[0-9]', ' 1234')\n",
    "print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "12281606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@안녕하세요 반갑습니다^^  의미없는 숫자...!!!\n",
      " 안녕하세요 반갑습니다 의미없는 숫자 \n"
     ]
    }
   ],
   "source": [
    "string = \"@안녕하세요 반갑습니다^^ 123 의미없는 숫자...!!!\"\n",
    "\n",
    "#숫자 데이터 제거\n",
    "p = re.compile('[0-9]+')\n",
    "result = p.sub('', string)\n",
    "print(result)\n",
    "\n",
    "p = re.compile(\"\\W+\")\n",
    "result = p.sub(' ', result)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4bc3df6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['안녕하세요 반갑습니다', 'My Job is Programmer', 'CC++ C Python']\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "import sys\n",
    "\n",
    "text_data = ['안녕하세요 반갑습니다.', 'My Job is Programmer', 'C&C++, C#, Python']\n",
    "\n",
    "#구두점 딕셔너리를 생성\n",
    "punctuation = dict.fromkeys(i for i in range(sys.maxunicode) \n",
    "                            if unicodedata.category(chr(i)).startswith('P'))\n",
    "result = [string.translate(punctuation) for string in text_data]\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f5fb33e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/m1/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to /Users/m1/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e79f67fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'science', 'of', 'today', 'is', 'the', 'technology', 'of', 'tomorrow']\n",
      "['품질은 양보다 중요합니다.', '한 번의 홈런이 두 번의 2루타보다 낫습니다.', '혁신은 현존하는 수천 가지 것들에 아니라고 말하는 것이다.', '시간이 없습니다 누군가를 위해서 당신의 삶을 버리지 마세요.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "string = \"The science of today is the technology of tomorrow\"\n",
    "print(word_tokenize(string))\n",
    "string = \"\"\"품질은 양보다 중요합니다. 한 번의 홈런이 두 번의 2루타보다 낫습니다. 혁신은 현존하는 수천 가지 것들에 아니라고 말하는 것이다.\n",
    "시간이 없습니다 누군가를 위해서 당신의 삶을 버리지 마세요.\"\"\"\n",
    "print(sent_tokenize(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c670f852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1월', '3월', '4월']\n",
      "['chief', 'president', 'kennedy']\n",
      "['chief', 'president', 'kennedy']\n"
     ]
    }
   ],
   "source": [
    "#불용어 제거 - 한글은 불용어 사전이 없어서 불용어 사전을 직접 작성\n",
    "word_korean = ['1월', '2월', '3월', '4월']\n",
    "stopwords = ['2월', '5월']\n",
    "\n",
    "#i for i in word_korean if i not in stopwords 는 작업을 수행해서 generator를 생성\n",
    "#generator는 이터레이터로 접근할 수 있는 객체\n",
    "print([i for i in word_korean if i not in stopwords])\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#영문은 nltk에서 기본적인 불용어 사전을 제공\n",
    "words_english = ['chief', 'the', 'an', 'and', 'president', 'kennedy', 'move']\n",
    "result = [w for w in word_english if not w in stopwords.words('english')]\n",
    "print(result)\n",
    "\n",
    "#sklearn이 nltk 보다 불용어의 개수가 조금 더 많음\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "result = [w for w in word_english if not w in ENGLISH_STOP_WORDS]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "464cf646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['All', 'pythoners', 'have', 'pythoned', 'poorly', 'at', 'least', 'once']\n",
      "al\tpython\thav\tpython\tpoor\tat\tleast\tont\t"
     ]
    }
   ],
   "source": [
    "#영문 어간 추출\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "string = \"All pythoners have pythoned poorly at least once\"\n",
    "\n",
    "#단어 토큰화 - 공백을 기준으로 분할해서 list로 생성\n",
    "words = word_tokenize(string)\n",
    "print(words)\n",
    "\n",
    "ps_stemmer = LancasterStemmer()\n",
    "\n",
    "#어간 추출\n",
    "for w in words:\n",
    "    print(ps_stemmer.stem(w), end=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f37391c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('little', 'JJ'), ('yellow', 'JJ'), ('don', 'NN'), ('barked', 'VBD'), ('at', 'IN'), ('the', 'DT'), ('Persian', 'JJ'), ('cat', 'NN')]\n",
      "['don', 'cat']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/m1/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "#형태소 분석 - 품사 태깅\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize\n",
    "\n",
    "tokens = word_tokenize('The little yellow don barked at the Persian cat')\n",
    "tags_en = pos_tag(tokens)\n",
    "#단어와 품사의 튜플의 리스트로 출력\n",
    "#품사(NNP-고유명사, NN:명사, RB:부사, VBD:동사, VBG:동사, 동명사, 현재분사, JJ: guddydtk)\n",
    "print(tags_en)\n",
    "#명사와 고유명사만 추출\n",
    "print([word for word, tag in tags_en if tag in ['NN', 'NNP']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "05eb38f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['태양계는 지금으로부터 약 46억 년 전 거대한 분자 구름의 일부분이 중력 붕괴를 일으키면서 형성되었다']\n",
      "['태양계', '지금', '46', '46억년', '억', '년', '전', '거대', '분자', '구름', '일부분', '중력', '붕괴', '형성']\n",
      "['태양계', '는', '지금', '으로', '부터', '약', '46', '억', '년', '전', '거대', '하', 'ㄴ', '분자', '구름', '의', '일부분', '이', '중력', '붕괴', '를', '일으키', '면서', '형성', '되', '었', '다']\n",
      "[('태양계', 'NNG'), ('는', 'JX'), ('지금', 'NNG'), ('으로', 'JKM'), ('부터', 'JX'), ('약', 'MDN'), ('46', 'NR'), ('억', 'NR'), ('년', 'NNB'), ('전', 'NNG'), ('거대', 'NNG'), ('하', 'XSV'), ('ㄴ', 'ETD'), ('분자', 'NNG'), ('구름', 'NNG'), ('의', 'JKG'), ('일부분', 'NNG'), ('이', 'JKS'), ('중력', 'NNG'), ('붕괴', 'NNG'), ('를', 'JKO'), ('일으키', 'VV'), ('면서', 'ECE'), ('형성', 'NNG'), ('되', 'XSV'), ('었', 'EPT'), ('다', 'EFN')]\n",
      "['태양계', '지금', '약', '46억년', '전', '거대', '자', '구름', '일부분', '중력', '붕괴', '형성']\n",
      "<konlpy.tag._hannanum.Hannanum object at 0x14a81d1d0>\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Kkma\n",
    "text = '''태양계는 지금으로부터 약 46억년 전 거대한 분자 구름의 일부분이 중력 붕괴를 일으키면서 형성되었다'''\n",
    "kkma = Kkma()\n",
    "\n",
    "#문장 분석\n",
    "print(kkma.sentences(text))\n",
    "#단어 분석\n",
    "print(kkma.nouns(text))\n",
    "#형태소 분석\n",
    "print(kkma.morphs(text))\n",
    "print(kkma.pos(text))\n",
    "\n",
    "#다른 형태소 분석기\n",
    "#성능은 Kkma가 우수하다고 알려져 있는데 메모리 사용량이 많고 속도가 조금 느립니다.\n",
    "from konlpy.tag import Hannanum\n",
    "hannanum = Hannanum()\n",
    "#단어 분석\n",
    "print(hannanum.nouns(text))\n",
    "#형태소 분석\n",
    "print(hannanum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "390761fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'love': 6, 'newzealand': 7, 'germany': 2, 'good': 3, 'beats': 0, 'both': 1, 'korea': 5, 'is': 4, 'top': 10, 'of': 8, 'the': 9}\n",
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.70710678 0.70710678 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.70710678 0.70710678 0.         0.         0.        ]\n",
      " [0.         0.         0.62791376 0.77828292 0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.61418897 0.61418897 0.49552379 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.35355339 0.35355339\n",
      "  0.         0.         0.35355339 0.35355339 0.70710678]]\n"
     ]
    }
   ],
   "source": [
    "#BoW(Bag of Word - 단어의 개수)\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "text_data = np.array([\n",
    "    'I love Newzealand',\n",
    "    'I Newzealand love',\n",
    "    'Germany good',\n",
    "    'Germany beats both',\n",
    "    'Korea is top of the top'\n",
    "])\n",
    "\n",
    "#tf-idf 객체를 생성\n",
    "tfidf = TfidfVectorizer()\n",
    "feature_matrix = tfidf.fit_transform(text_data)\n",
    "\n",
    "#print(feature_matrix)\n",
    "print(tfidf.vocabulary_)\n",
    "print(feature_matrix.toarray())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
